{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cf660ca-d240-4af5-8afe-6bf5806facc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Taraneh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score: 0.3258075286277704\n",
      "Topic: 0 \n",
      "Words: 0.090*\"co\" + 0.089*\"https\" + 0.025*\"covid\" + 0.017*\"uber\" + 0.008*\"coronavirus\" + 0.007*\"lyft\" + 0.007*\"flight\" + 0.005*\"bus\" + 0.005*\"bike\" + 0.005*\"airline\"\n",
      "Topic: 1 \n",
      "Words: 0.089*\"co\" + 0.089*\"https\" + 0.038*\"uber\" + 0.023*\"covid\" + 0.020*\"drivers\" + 0.016*\"bike\" + 0.016*\"coronavirus\" + 0.011*\"lyft\" + 0.011*\"face\" + 0.010*\"masks\"\n",
      "Topic: 2 \n",
      "Words: 0.090*\"co\" + 0.090*\"https\" + 0.029*\"covid\" + 0.028*\"airline\" + 0.013*\"aviation\" + 0.013*\"industry\" + 0.010*\"airport\" + 0.010*\"news\" + 0.009*\"latest\" + 0.008*\"keep\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to load data from a JSONL file\n",
    "def load_data(file_path):\n",
    "    tweets = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tweet = json.loads(line)\n",
    "            tweets.append(tweet['text'])  # Assuming 'text' is the field containing the tweet text\n",
    "    return tweets\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_texts(texts):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# Update these file paths according to your data files' location\n",
    "file_paths = [\n",
    "    r'C:\\Users\\Taraneh\\OneDrive - North Dakota University System\\Desktop\\New folder (3)\\TweetData_JSONL_Starting2020_04_01_To2020_04_30_Retrieved27082020_By1133.jsonl',\n",
    "    r'C:\\Users\\Taraneh\\OneDrive - North Dakota University System\\Desktop\\New folder (3)\\TweetData_JSONL_Starting2020_05_01_To2020_05_31_Retrieved27082020_By1136.jsonl',\n",
    "    r'C:\\Users\\Taraneh\\OneDrive - North Dakota University System\\Desktop\\New folder (3)\\TweetData_JSONL_Starting2020_06_01_To2020_06_30_Retrieved27082020_By1137.jsonl',\n",
    "    r'C:\\Users\\Taraneh\\OneDrive - North Dakota University System\\Desktop\\New folder (3)\\TweetData_JSONL_Starting2020_07_01_To2020_07_31_Retrieved27082020_By1138.jsonl',\n",
    "    r'C:\\Users\\Taraneh\\OneDrive - North Dakota University System\\Desktop\\New folder (3)\\TweetData_JSONL_Starting2020_08_01_To2020_08_27_Retrieved27082020_By1139.jsonl'\n",
    "]\n",
    "\n",
    "tweets = []\n",
    "for file_path in file_paths:\n",
    "    tweets.extend(load_data(file_path))\n",
    "\n",
    "preprocessed_texts = preprocess_texts(tweets)\n",
    "\n",
    "# Create Dictionary and Corpus\n",
    "id2word = corpora.Dictionary(preprocessed_texts)\n",
    "corpus = [id2word.doc2bow(text) for text in preprocessed_texts]\n",
    "\n",
    "# Parameters for LDA\n",
    "num_topics = 3\n",
    "alpha = 0.01\n",
    "eta = 0.01\n",
    "passes = 50\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=passes,\n",
    "                                       alpha=alpha,\n",
    "                                       eta=eta)\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score:', coherence_lda)\n",
    "\n",
    "# Explore the topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fac8332-5f1e-418f-b910-79a70dbdd71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Taraneh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score: 0.37193319898806626\n",
      "Topic: 0 \n",
      "Words: 0.049*\"covid\" + 0.029*\"airline\" + 0.017*\"flight\" + 0.015*\"pandemic\" + 0.014*\"coronavirus\" + 0.013*\"aviation\" + 0.013*\"industry\" + 0.011*\"news\" + 0.011*\"subway\" + 0.011*\"keep\"\n",
      "Topic: 1 \n",
      "Words: 0.079*\"uber\" + 0.046*\"covid\" + 0.036*\"drivers\" + 0.029*\"lyft\" + 0.022*\"coronavirus\" + 0.017*\"mask\" + 0.015*\"masks\" + 0.013*\"wear\" + 0.013*\"driver\" + 0.012*\"service\"\n",
      "Topic: 2 \n",
      "Words: 0.056*\"covid\" + 0.039*\"airline\" + 0.034*\"thank\" + 0.022*\"critical\" + 0.020*\"emergency\" + 0.020*\"fund\" + 0.019*\"demand\" + 0.018*\"whitehouse\" + 0.018*\"cares\" + 0.018*\"uscongress\"\n",
      "Topic: 3 \n",
      "Words: 0.055*\"bike\" + 0.028*\"social\" + 0.024*\"ride\" + 0.023*\"free\" + 0.021*\"bicycles\" + 0.021*\"distancing\" + 0.019*\"exercise\" + 0.018*\"change\" + 0.017*\"people\" + 0.017*\"great\"\n",
      "Topic: 4 \n",
      "Words: 0.043*\"uber\" + 0.029*\"similar\" + 0.028*\"tesla\" + 0.028*\"market\" + 0.027*\"university\" + 0.026*\"disrupted\" + 0.026*\"phoenix\" + 0.022*\"consider\" + 0.020*\"services\" + 0.020*\"suggest\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Improved preprocessing function\n",
    "def preprocess_texts(texts):\n",
    "    stop_words = stopwords.words('english')\n",
    "    # Additional stopwords can be added to this list\n",
    "    stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)  # Remove URLs\n",
    "        text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "        text = re.sub(r\"\\'\", \"\", text)  # Remove single quotes\n",
    "        text = gensim.utils.simple_preprocess(str(text), deacc=True)  # De-accent and simple preprocess\n",
    "        texts_out.append([word for word in text if word not in stop_words and len(word) > 3])  # Remove stopwords and short words\n",
    "    return texts_out\n",
    "\n",
    "# Load data (assuming tweets have been loaded into the `tweets` list as before)\n",
    "preprocessed_texts = preprocess_texts(tweets)\n",
    "\n",
    "# Create Dictionary and Corpus, including filtering extremes\n",
    "id2word = corpora.Dictionary(preprocessed_texts)\n",
    "id2word.filter_extremes(no_below=5, no_above=0.5)  # Filter out words in less than 5 documents or in more than 50% of the documents\n",
    "corpus = [id2word.doc2bow(text) for text in preprocessed_texts]\n",
    "\n",
    "# Train LDA model with new parameters\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=5,  # Trying a different number of topics\n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,  # Reduced number of passes for quicker iteration\n",
    "                                       alpha='asymmetric',  # Different alpha setting\n",
    "                                       eta='auto')  # Auto-tune eta\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score:', coherence_lda)\n",
    "\n",
    "# Explore the topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7eb43f-8ab2-4b06-85a8-4603b8caad58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
